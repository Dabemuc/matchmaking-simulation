This is a **very strong metrics setup** already. Youâ€™re clearly thinking like a systems engineer, not just â€œadd some countersâ€. Iâ€™ll break this down into:

1. **Overall assessment**
2. **Whatâ€™s missing / what Iâ€™d tweak**
3. **Subtle correctness issues**
4. **Dashboards I would build (very concrete)**
5. **One optional â€œnext-levelâ€ improvement**

---

## 1ï¸âƒ£ Overall assessment

Short version: **this is production-grade for a simulation harness**.

You have:

- Correct metric _types_ (gauges vs counters vs histograms)
- Bounded cardinality
- Metrics aligned with actual control points (pool, compositor, scenarios)
- Backpressure visibility
- Cancellation & shutdown observability
- Self-observability (goroutines, tick durations)

This is **better than what many real services ship**.

---

## 2ï¸âƒ£ Whatâ€™s missing / what I would add

### ğŸ”¹ A) Scenario _attempted_ vs _executed_

Right now you have:

- `scenario_started_total` â†’ when a player actually runs it
- `compositor_idle_starvation_total` â†’ when scheduling fails

Whatâ€™s missing is an explicit **â€œattempted executionsâ€** metric.

#### Why it matters

You want to see:

```
attempted > started â†’ starvation
attempted == started â†’ healthy
```

### Suggested metric

```go
scenarioAttemptedTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Namespace: "harness",
        Name:      "scenario_attempted_total",
        Help:      "Scenario executions requested by compositor.",
    },
    []string{"scenario"},
)
```

Increment this **before** calling `ExecuteScenario`.

---

### ğŸ”¹ B) Pool capacity metric

You expose:

- `pool_idle_queue_depth`

But not:

- **maximum idle capacity**

Without it, dashboards canâ€™t show saturation percentage.

### Suggested metric

```go
poolIdleCapacity = promauto.NewGauge(
    prometheus.GaugeOpts{
        Namespace: "harness",
        Name:      "pool_idle_capacity",
        Help:      "Maximum idle pool capacity.",
    },
)
```

Set once in `New()`.

---

### ğŸ”¹ C) Player creation rate

You have total and active, but not **rate**.

PromQL can derive it, but itâ€™s a _very important_ signal.

No new metric needed, but you should plan to graph:

```promql
rate(harness_players_total[5s])
```

Iâ€™d explicitly plan a dashboard panel for this.

---

### ğŸ”¹ D) Scenario cancellation vs failure distinction

Currently:

```go
if err != nil {
    scenarioCompletedTotal.WithLabelValues(s.Name(), "failure").Inc()
}
```

But `ctx.Err()` during shutdown is not really a â€œfailureâ€.

### Suggested statuses

- `success`
- `failure`
- `cancelled`

This makes shutdown behavior _much_ clearer.

---

### ğŸ”¹ E) Compositor tick drift

You measure tick duration (excellent), but not **tick skew**.

If ticks are supposed to be 1s and start drifting, you wonâ€™t see it.

Optional metric:

```go
compositorTickLagSeconds
```

But this is optional unless you expect extreme load.

---

## 3ï¸âƒ£ Subtle correctness issues (important)

These arenâ€™t conceptual problems â€” just things to tighten.

---

### âš ï¸ `playersIdle` gauge correctness

You do:

```go
playersIdle.Inc()   // when putting into idle
playersIdle.Dec()   // when taking from idle
```

This is correct **only if**:

- Every `idle <- p` is matched with exactly one take
- No player exits while idle without decrement

But in this block:

```go
case idle <- p:
    playersIdle.Inc()
case <-ctx.Done():
    return
}
```

If context cancels **after** player is idle but **before** pool pulls it, you leak the gauge.

#### Safer pattern

Track idle count from `len(p.idle)` in the monitor loop only and **remove manual Inc/Dec**.

You already have:

```go
poolIdleQueueDepth.Set(float64(len(p.idle)))
```

I would **delete `playersIdle` entirely** and rely on that.

---

### âš ï¸ `playerCnt` is not atomic

`PlayerCount()` returns `p.playerCnt`, but itâ€™s written concurrently.

Prometheus reads donâ€™t care, but correctness does.

Fix:

- Either make it atomic
- Or derive active players from `playersActive` gauge

Iâ€™d remove `PlayerCount()` entirely and base compositor logic on `playersActive`.

---

## 4ï¸âƒ£ Dashboards I would build

Hereâ€™s the **exact dashboard structure** Iâ€™d ship.

---

## ğŸ“Š Dashboard 1: â€œSystem Overviewâ€

**Row: Load**

- Active Players â†’ `harness_players_active`
- Player Creation Rate â†’ `rate(harness_players_total[5s])`

**Row: Pool Health**

- Idle Pool Depth â†’ `harness_pool_idle_queue_depth`
- Idle Saturation â†’

```promql
harness_pool_idle_queue_depth / harness_pool_idle_capacity
```

**Row: Errors**

- Internal Errors â†’

```promql
rate(harness_errors_total[5m])
```

---

## ğŸ“Š Dashboard 2: â€œScenario Throughputâ€

For each scenario (templated variable):

**Panels**

- Desired Rate â†’ `harness_compositor_desired_rate`
- Attempted vs Started â†’

```promql
rate(harness_scenario_attempted_total[5s])
rate(harness_scenario_started_total[5s])
```

- Completions â†’

```promql
rate(harness_scenario_completed_total{status="success"}[5s])
```

This tells you instantly if scheduling is working.

---

## ğŸ“Š Dashboard 3: â€œScenario Latencyâ€

Per scenario:

- P50 / P95 / P99 â†’

```promql
histogram_quantile(
  0.95,
  rate(harness_scenario_duration_seconds_bucket[5m])
)
```

Overlay all scenarios or split by panel.

---

## ğŸ“Š Dashboard 4: â€œBackpressure & Saturationâ€

This is the _money dashboard_.

- Idle Starvation Rate â†’

```promql
rate(harness_compositor_idle_starvation_total[5s])
```

- Pool Execute Wait P95 â†’

```promql
histogram_quantile(
  0.95,
  rate(harness_pool_execute_wait_duration_seconds_bucket[5m])
)
```

If these go up â†’ system overloaded.

---

## ğŸ“Š Dashboard 5: â€œScheduler Healthâ€

- Tick Duration (pool & compositor)
- Goroutines
- Context Cancellations by source

This answers:

> â€œIs my harness itself breaking?â€
